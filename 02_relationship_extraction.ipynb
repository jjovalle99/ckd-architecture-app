{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.cache import SQLiteCache\n",
    "from langchain.globals import set_llm_cache\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_text_splitters import TokenTextSplitter\n",
    "from nest_asyncio import apply as nest_asyncio_apply\n",
    "\n",
    "_ = nest_asyncio_apply(), load_dotenv()\n",
    "set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n",
    "DATA = Path(\"data\")\n",
    "random.seed(1399)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Extractor LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-4 Turbo\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4-turbo-2024-04-09\",\n",
    "    max_tokens=512,\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm here to help you. What's on your mind today?"
     ]
    }
   ],
   "source": [
    "# Inference test\n",
    "async for tokens in llm.astream(\"Hey king, how is it going?\"):\n",
    "    print(tokens.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load parsed documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284\n"
     ]
    }
   ],
   "source": [
    "# Load parsed pdfs (since these were loaded as LlamaIndex Documents, the library is needed to convert them to LC Documents)\n",
    "with open(DATA / \"parsed_docs.pkl\", \"rb\") as f:\n",
    "    documents = pickle.load(f)\n",
    "\n",
    "# Convert parsed pdfs to LC documents\n",
    "documents = [Document(page_content=doc.text) for doc in documents]\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split documents -> brute force approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2437"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = TokenTextSplitter(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=20,\n",
    ")\n",
    "splitted_documents = text_splitter.split_documents(documents=documents)\n",
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load list of AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Amazon A2I, Amazon API Gateway, Amazon AppFlow, Amazon AppStream 2.0, Amazon Athena, Amazon Aurora, '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parsed pdfs\n",
    "with open(DATA / \"services_data_sc.json\", \"r\") as f:\n",
    "    services = json.load(f)\n",
    "    services = [service[\"service_name\"] for service in services]\n",
    "    services = \", \".join(services)\n",
    "services[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define extractor prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the structure of the expected output\n",
    "class AWSServicesRelationship(BaseModel):\n",
    "    \"\"\"Relantionship between two AWS services. The relationship represents a interaction between two AWS services in an AWS architecture.\"\"\"\n",
    "    serviceA: Optional[str] = Field(description=\"Name of the first AWS service.\")\n",
    "    serviceB: Optional[str] = Field(description=\"Name of the second AWS service.\")\n",
    "    relationship: Optional[str] = Field(description=\"Description of the relationship between the two services. This must be a single word.\")\n",
    "    evidence: Optional[str] = Field(description=\"Verbatim sentence of the text where the relationship is mentioned.\")\n",
    "\n",
    "class DataExtracted(BaseModel):\n",
    "    \"\"\"Relantionship between two AWS services. The relationship represents a connection between two AWS services in an AWS architecture.\"\"\"\n",
    "    relationships: List[AWSServicesRelationship] = Field(description=\"List of relationships between AWS services.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompt\n",
    "system_prompt = f\"\"\"\n",
    "As an expert in analyzing cloud architectures, your task is to identify and extract relationships between AWS services from the provided text. A 'relationship' is defined as any instance where one AWS service directly interacts with, supports, or is configured to use another AWS service.\n",
    "\n",
    "Please follow these steps:\n",
    "1. Read the text carefully to identify any sentences that describe a relationship between the listed AWS services.\n",
    "2. Extract the names of the two AWS services involved in the relationship.\n",
    "3. In one word only, capture the type of relationship between the two services.\n",
    "4. Capture the sentence or fragment that best describes the relationship as evidence.\n",
    "5. Focus solely on extracting relationships between the following AWS services:\n",
    "<services> {services} </services>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jjmov99/escala247/cdk-architecture-app/.venv/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "# Define prompt and extractor runnable\n",
    "messages = [\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages=messages)\n",
    "extractor = prompt | llm.with_structured_output(\n",
    "    schema=DataExtracted,\n",
    "    method=\"function_calling\",\n",
    "    include_raw=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing a sample & measuring cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"relationships\": [\n",
      "        {\n",
      "            \"serviceA\": \"AWS Organizations\",\n",
      "            \"serviceB\": \"Amazon EC2\",\n",
      "            \"relationship\": \"governance\",\n",
      "            \"evidence\": \"customers should use a dedicated AWS Account that is governed by AWS Organizations for running the SWIFT secure zone in a production environment.\"\n",
      "        },\n",
      "        {\n",
      "            \"serviceA\": \"AWS Config\",\n",
      "            \"serviceB\": \"AWS CloudTrail\",\n",
      "            \"relationship\": \"monitoring\",\n",
      "            \"evidence\": \"you can monitor AWS CloudTrail for any unintended changes.\"\n",
      "        },\n",
      "        {\n",
      "            \"serviceA\": \"AWS CloudFormation\",\n",
      "            \"serviceB\": \"AWS Config\",\n",
      "            \"relationship\": \"integration\",\n",
      "            \"evidence\": \"you can use AWS Config to help you detect drifts and alterations that have happened in the SWIFT secure zone, and you can monitor AWS CloudTrail for any unintended changes.\"\n",
      "        },\n",
      "        {\n",
      "            \"serviceA\": \"AWS Control Tower\",\n",
      "            \"serviceB\": \"Amazon VPC\",\n",
      "            \"relationship\": \"management\",\n",
      "            \"evidence\": \"AWS offers AWS Control Tower, a managed service that sets up a landing zone based on multi-account best practices, centralizes identity and access management, and establishes pre-configured governance rules for security and compliance.\"\n",
      "        },\n",
      "        {\n",
      "            \"serviceA\": \"Amazon WorkSpaces\",\n",
      "            \"serviceB\": \"Amazon VPC\",\n",
      "            \"relationship\": \"deployment\",\n",
      "            \"evidence\": \"This Amazon Workspace virtual desktop would be deployed to the secure zone VPC and act as a bastion host to access the rest of the secure zone\\u2019s components.\"\n",
      "        },\n",
      "        {\n",
      "            \"serviceA\": \"Amazon EC2\",\n",
      "            \"serviceB\": \"Amazon EC2 Image Builder\",\n",
      "            \"relationship\": \"utilization\",\n",
      "            \"evidence\": \"You can leverage Amazon EC2 Image Builder as part of the AMI pipeline implementation for building the SWIFT application AMIs.\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result = extractor.invoke({\"input\": splitted_documents[1000].page_content})\n",
    "print(result.json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total expected cost (high approximation): 13.40 USD\n"
     ]
    }
   ],
   "source": [
    "# GPT 4 Turbo new prices\n",
    "input_ppt = 0.000001\n",
    "output_ppt = 0.000003\n",
    "input_tokens = 4000\n",
    "output_tokens = 500\n",
    "cost = (input_ppt * input_tokens) + (output_ppt * output_tokens)\n",
    "print(f\"Total expected cost (high approximation): {cost * len(splitted_documents):.2f} USD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of total tokens: 10,966,500\n"
     ]
    }
   ],
   "source": [
    "# Token limits\n",
    "total_tokens_per_request = input_tokens + output_tokens\n",
    "print(f\"Expected number of total tokens: {total_tokens_per_request * len(splitted_documents):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux function to get batches\n",
    "def gen_batches(iterable, n):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_batch = [{\"input\": chunk.page_content} for chunk in splitted_documents]\n",
    "len(to_batch) == len(splitted_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "# total = len(to_batch) // batch_size + 1\n",
    "# relationships = []\n",
    "\n",
    "# for idx, batch in enumerate(gen_batches(iterable=to_batch, n=batch_size), start=1):\n",
    "\n",
    "#     print(f\"Batch {idx}/{total} processing...\")\n",
    "#     _tmp = await extractor.abatch(inputs=batch, return_exceptions=True)\n",
    "#     relationships.append(_tmp)\n",
    "#     print(f\"Batch {idx}/{total} done! Sleeping...\")\n",
    "\n",
    "#     await asyncio.sleep(62)\n",
    "#     print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "total = len(to_batch) // batch_size + 1\n",
    "batches = list(gen_batches(iterable=to_batch, n=batch_size))\n",
    "len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[0], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[1], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[2], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[3], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[4], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[5], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[6], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[7], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[8], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[9], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[10], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[11], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[12], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[13], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[14], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[15], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[16], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[17], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[18], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[19], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationships.extend(await extractor.abatch(inputs=batches[20], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA / \"relationships.pkl\", \"wb\") as f:\n",
    "    pickle.dump(relationships, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationships.extend(await extractor.abatch(inputs=batches[21], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationships.extend(await extractor.abatch(inputs=batches[22], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationships.extend(await extractor.abatch(inputs=batches[23], return_exceptions=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relationships.extend(await extractor.abatch(inputs=batches[24], return_exceptions=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
